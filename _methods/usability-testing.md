---
layout: card
title: Usability testing
permalink: /validation/usability-testing/
redirect_from:
  - /usability-testing/
description: Observing users as they attempt to use a product or service while thinking out loud.
category: Validation
what: Observing users as they attempt to use a product or service while thinking out loud.
why: To better understand how intuitive the team's design is, and how adaptable it is to meeting user needs.
timeRequired: 30 minutes to 1 hour per test
---

## How to do it

1. Pick what you’ll test. Choose something, such as a sketch, [prototype]({{site.baseurl}}/prototyping/), or even a "competitor's product" that might help users accomplish their goals.
1. Plan the test. Schedule a research-planning meeting and invite anyone who has an interest in what you’d like to test (using your discretion, of course). Align the group on the scenarios the test will center around, which users should participate (and how you'll [recruit]({{site.baseurl}}/recruiting/) them), and which members of your team will moderate and observe. Prepare a usability test script ([example]({{site.baseurl}}/usability-test-script/)).
1. Recruit users and inform their consent. Provide a way for potential participants to sign up for the test. Pass along to participants an [agreement]({{site.baseurl}}/participant-agreement/) explaining what participation will entail. Clarify any logistical expectations, such as screen sharing, and pass along links or files of whatever it is you're testing.
1. Run the tests. Moderators should verbally confirm with the participant that it’s okay to record the test, ask participants to think outloud, and otherwise remain silent. Observers should contribute to a [rolling issues log]({{site.baseurl}}/rolling-issues-log/). Engage your team in a [post-interview debrief]({{site.baseurl}}/interview-debrief/) after each test.
1. Discuss the results. Schedule a 90-minute collaborative synthesis meeting to discuss issues you observed, and any questions these tests raise concerning user needs. Conclude the meeting by determining how the team will use what it learned in service of future design decisions.

<section class="method--section method--section--18f-example" markdown="1" >

## Example from 18F

- [Usability testing plans from 18F's Extractive Industries Transparency Initiative project with Department of the Interior](https://github.com/18F/doi-extractives-data/tree/research/research)
- [Introduction to remote moderated usability testing, part 1&#58; What and why](https://18f.gsa.gov/2018/11/14/introduction-to-remote-moderated-usability-testing-part-1/)
- [Introduction to remote moderated usability testing, part 2&#58; How](https://18f.gsa.gov/2018/11/20/introduction-to-remote-moderated-usability-testing-part-2-how/)


</section>

<section class="method--section method--section--additional-resources" markdown="1">

## Additional resources

- Example Design Research Participant Agreement ([English]({{site.baseurl}}/participant-agreement/)/[Spanish]({{site.baseurl}}/participant-agreement-spanish/))
- [Interview checklist]({{site.baseurl}}/interview-checklist/)
- [Example usability test script]({{site.baseurl}}/usability-test-script/)
- [Rolling issues log]({{site.baseurl}}/rolling-issues-log/)

</section>

<section class="method--section method--section--government-considerations" markdown="1" >

## Considerations for use in government  

No PRA implications. First, any given usability test should involve nine or fewer users. Additionally, the PRA explicitly exempts direct observation and non-standardized conversation, 5 CFR 1320.3(h)3. It also specifically excludes tests of knowledge or aptitude, 5 CFR 1320.3(h)7, which is essentially what a usability test tests. See the methods for [Recruiting](/recruiting/) and [Privacy](/privacy/) for more tips on taking input from the public.
</section>
